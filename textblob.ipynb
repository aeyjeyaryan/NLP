{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aeyjeyaryan/NLP/blob/main/textblob.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5946a186",
      "metadata": {
        "id": "5946a186",
        "outputId": "5002672f-ddc2-48b8-99d8-21b760fb0f84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in c:\\users\\sahil\\anaconda3\\lib\\site-packages (3.8.1)\n",
            "Requirement already satisfied: click in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
            "Requirement already satisfied: joblib in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
            "Requirement already satisfied: tqdm in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
            "Requirement already satisfied: textblob in c:\\users\\sahil\\anaconda3\\lib\\site-packages (0.18.0.post0)\n",
            "Requirement already satisfied: nltk>=3.8 in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from textblob) (3.8.1)\n",
            "Requirement already satisfied: click in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (8.0.4)\n",
            "Requirement already satisfied: joblib in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (2022.7.9)\n",
            "Requirement already satisfied: tqdm in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (4.65.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from click->nltk>=3.8->textblob) (0.4.6)\n"
          ]
        }
      ],
      "source": [
        "### Install Textblob\n",
        "!pip install nltk\n",
        "!pip install textblob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53678d74",
      "metadata": {
        "id": "53678d74",
        "outputId": "62cb0524-2f9f-442d-cea6-ca983d46cdbb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to\n",
            "[nltk_data]    |     C:\\Users\\sahil\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to\n",
            "[nltk_data]    |     C:\\Users\\sahil\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to\n",
            "[nltk_data]    |     C:\\Users\\sahil\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to\n",
            "[nltk_data]    |     C:\\Users\\sahil\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to\n",
            "[nltk_data]    |     C:\\Users\\sahil\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     C:\\Users\\sahil\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to\n",
            "[nltk_data]    |     C:\\Users\\sahil\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to\n",
            "[nltk_data]    |     C:\\Users\\sahil\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to\n",
            "[nltk_data]    |     C:\\Users\\sahil\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to\n",
            "[nltk_data]    |     C:\\Users\\sahil\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     C:\\Users\\sahil\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to\n",
            "[nltk_data]    |     C:\\Users\\sahil\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to\n",
            "[nltk_data]    |     C:\\Users\\sahil\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to\n",
            "[nltk_data]    |     C:\\Users\\sahil\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to\n",
            "[nltk_data]    |     C:\\Users\\sahil\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to\n",
            "[nltk_data]    |     C:\\Users\\sahil\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to\n",
            "[nltk_data]    |     C:\\Users\\sahil\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to\n",
            "[nltk_data]    |     C:\\Users\\sahil\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     C:\\Users\\sahil\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to\n",
            "[nltk_data]    |     C:\\Users\\sahil\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     C:\\Users\\sahil\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     C:\\Users\\sahil\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('popular')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54facb1a",
      "metadata": {
        "id": "54facb1a",
        "outputId": "54008e59-1f42-4634-dc7a-d0e9ac6aaf2e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\sahil\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "057179ae",
      "metadata": {
        "id": "057179ae",
        "outputId": "ffd13c1e-1ef1-4381-be2a-26902c0d15f5"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'TextBlob' object has no attribute 'detect_language'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[5], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtextblob\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextBlob\n\u001b[0;32m      3\u001b[0m blob \u001b[38;5;241m=\u001b[39m TextBlob(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHey John, How are You\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDetected Language is:\u001b[39m\u001b[38;5;124m\"\u001b[39m,blob\u001b[38;5;241m.\u001b[39mdetect_language())\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput text in Spanish:\u001b[39m\u001b[38;5;124m\"\u001b[39m,blob\u001b[38;5;241m.\u001b[39mtranslate(to\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mes\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'TextBlob' object has no attribute 'detect_language'"
          ]
        }
      ],
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "blob = TextBlob(\"Hey John, How are You\")\n",
        "\n",
        "print(\"Detected Language is:\",blob.detect_language())\n",
        "\n",
        "print(\"Input text in Spanish:\",blob.translate(to='es'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c244158",
      "metadata": {
        "id": "3c244158"
      },
      "outputs": [],
      "source": [
        "#spelling check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e5276e6",
      "metadata": {
        "id": "9e5276e6"
      },
      "outputs": [],
      "source": [
        "from textblob import TextBlob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "848a3b00",
      "metadata": {
        "id": "848a3b00"
      },
      "outputs": [],
      "source": [
        "text = 'Aplple Corporation prvides best srevices in teh market'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28d3715d",
      "metadata": {
        "id": "28d3715d"
      },
      "outputs": [],
      "source": [
        "blob = TextBlob(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "875e5818",
      "metadata": {
        "id": "875e5818",
        "outputId": "8f82e017-3746-4b9a-93e1-e127148bc00a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TextBlob(\"Apple Corporation provides best services in the market\")"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "blob.correct()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "724cd3e7",
      "metadata": {
        "id": "724cd3e7",
        "outputId": "e39e15e8-add3-44d1-8bc1-54933fd8f01a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TextBlob(\"gives\")"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "TextBlob('givess').correct()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "596c4cd3",
      "metadata": {
        "id": "596c4cd3"
      },
      "outputs": [],
      "source": [
        "#word count\n",
        "text = 'This is a random piece of text. Text can be of any type. Texts are basically mixtures of different words and letters together'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8977c5cb",
      "metadata": {
        "id": "8977c5cb"
      },
      "outputs": [],
      "source": [
        "blob = TextBlob(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f75e997",
      "metadata": {
        "id": "2f75e997",
        "outputId": "fe449be8-7a9f-4929-822e-3ce5b6c2de30"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "blob.word_counts['of']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e420b1a",
      "metadata": {
        "id": "5e420b1a"
      },
      "outputs": [],
      "source": [
        "#POS tagging\n",
        "#with tagging in textblob we can get tag of each words i.e noun pronoun adjective etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46a1fbc0",
      "metadata": {
        "id": "46a1fbc0",
        "outputId": "742c347c-8c1a-4e46-cb7f-3eed42fbb2bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('My', 'PRP$'), ('name', 'NN'), ('is', 'VBZ'), ('Aryan', 'NNP'), ('I', 'PRP'), ('like', 'VBP'), ('reading', 'VBG'), ('books', 'NNS'), ('The', 'DT'), ('recent', 'JJ'), ('book', 'NN'), ('I', 'PRP'), ('read', 'VBP'), ('was', 'VBD'), ('Ijoriya', 'NNP')]\n"
          ]
        }
      ],
      "source": [
        "text3 = TextBlob(\"My name is Aryan. I like reading books. The recent book I read was Ijoriya\")\n",
        "print(text3.tags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0ec4ade",
      "metadata": {
        "id": "e0ec4ade"
      },
      "outputs": [],
      "source": [
        "new_tuple = []\n",
        "for i in text3.tags:\n",
        "    if 'PRP' not in i[1]:\n",
        "        new_tuple.append(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7aad04b6",
      "metadata": {
        "id": "7aad04b6",
        "outputId": "86800421-2a50-4274-ed68-fcc6b8bc9d6d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('name', 'NN'),\n",
              " ('is', 'VBZ'),\n",
              " ('Aryan', 'NNP'),\n",
              " ('like', 'VBP'),\n",
              " ('reading', 'VBG'),\n",
              " ('books', 'NNS'),\n",
              " ('The', 'DT'),\n",
              " ('recent', 'JJ'),\n",
              " ('book', 'NN'),\n",
              " ('read', 'VBP'),\n",
              " ('was', 'VBD'),\n",
              " ('Ijoriya', 'NNP')]"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "new_tuple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "278a14e4",
      "metadata": {
        "id": "278a14e4"
      },
      "outputs": [],
      "source": [
        "#joining the above words\n",
        "join = ' '\n",
        "for i in new_tuple:\n",
        "    join = join+\" \"+\"\".join(i[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f9f78c9",
      "metadata": {
        "id": "6f9f78c9",
        "outputId": "434a4a47-1e7b-419c-d908-dc35a1ab29b5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'  name is Aryan like reading books The recent book read was Ijoriya'"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "join"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc027bd4",
      "metadata": {
        "id": "dc027bd4"
      },
      "outputs": [],
      "source": [
        "#tokenization with textblob\n",
        "text = '''Standard English: Despite its informality, “Hi” is a standard part of the English language and not slang. Slang words are often regional and may not be widely recognized or understood, which is not the case with “Hi.” '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93b595c3",
      "metadata": {
        "id": "93b595c3"
      },
      "outputs": [],
      "source": [
        "blob_object = TextBlob(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48d0ba86",
      "metadata": {
        "id": "48d0ba86"
      },
      "outputs": [],
      "source": [
        "tokens = blob_object.words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c28a588",
      "metadata": {
        "id": "3c28a588",
        "outputId": "16e70071-d7bb-428d-a07b-0a23b157936d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "WordList(['Standard', 'English', 'Despite', 'its', 'informality', '“', 'Hi', '”', 'is', 'a', 'standard', 'part', 'of', 'the', 'English', 'language', 'and', 'not', 'slang', 'Slang', 'words', 'are', 'often', 'regional', 'and', 'may', 'not', 'be', 'widely', 'recognized', 'or', 'understood', 'which', 'is', 'not', 'the', 'case', 'with', '“', 'Hi', '”'])"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a2b3e88",
      "metadata": {
        "id": "2a2b3e88",
        "outputId": "d173791c-e183-4691-a524-69b6e373cc65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "41\n"
          ]
        }
      ],
      "source": [
        "print(len(tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38955cb7",
      "metadata": {
        "id": "38955cb7"
      },
      "outputs": [],
      "source": [
        "token_sent = blob_object.sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92130cf5",
      "metadata": {
        "id": "92130cf5",
        "outputId": "a61fdf6d-ae58-4806-d746-ae168aed4794"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Sentence(\"Standard English: Despite its informality, “Hi” is a standard part of the English language and not slang.\"),\n",
              " Sentence(\"Slang words are often regional and may not be widely recognized or understood, which is not the case with “Hi.”\")]"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "token_sent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90fe8b88",
      "metadata": {
        "id": "90fe8b88",
        "outputId": "6d5720b5-ab81-4617-be72-b8bd9322bb3a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Trains'"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#pluralization(making a singular word to plural)\n",
        "from textblob import Word\n",
        "w = Word('Train')\n",
        "w.pluralize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bedffee3",
      "metadata": {
        "id": "bedffee3",
        "outputId": "d3d90ee1-87ff-4e17-bf04-cf8dbbddc166"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "boys\n",
            "books\n",
            "books\n",
            "books\n",
            "books\n"
          ]
        }
      ],
      "source": [
        "x = TextBlob(\"Aryan is a very good boy! He likes to read book. He is reading a book. He is not reading a comic book. It is a sci-fi book.\")\n",
        "for words,pos in x.tags:\n",
        "    if pos == 'NN':\n",
        "        print(words.pluralize())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3a2fdea",
      "metadata": {
        "id": "a3a2fdea",
        "outputId": "92a0e551-9c9f-46b0-96ca-a93b1f448d99"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'lemmatizing'"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Lemmatization using text blob\n",
        "z = Word('lemmatizing')\n",
        "z.lemmatize(\"n\") #n means noun, v can be verb and so on"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a31ba95d",
      "metadata": {
        "id": "a31ba95d",
        "outputId": "54a89469-05b2-4164-bdad-e73798796f74"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'lemmatizing'"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "z.lemmatize(\"v\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bbc2439",
      "metadata": {
        "id": "0bbc2439",
        "outputId": "a8b56866-2b56-4657-c1ab-80d60252ac18"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TextBlob(\"This is a random piece of text. Text can be of any type. Texts are basically mixtures of different words and letters together\")"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#ngrams in textblob\n",
        "# it creates a list of words which are called unigram, bigram , trigram etc\n",
        "blob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad1280d3",
      "metadata": {
        "scrolled": true,
        "id": "ad1280d3",
        "outputId": "721ec3c9-2957-4588-abf9-d50939a9ce9e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[WordList(['This']),\n",
              " WordList(['is']),\n",
              " WordList(['a']),\n",
              " WordList(['random']),\n",
              " WordList(['piece']),\n",
              " WordList(['of']),\n",
              " WordList(['text']),\n",
              " WordList(['Text']),\n",
              " WordList(['can']),\n",
              " WordList(['be']),\n",
              " WordList(['of']),\n",
              " WordList(['any']),\n",
              " WordList(['type']),\n",
              " WordList(['Texts']),\n",
              " WordList(['are']),\n",
              " WordList(['basically']),\n",
              " WordList(['mixtures']),\n",
              " WordList(['of']),\n",
              " WordList(['different']),\n",
              " WordList(['words']),\n",
              " WordList(['and']),\n",
              " WordList(['letters']),\n",
              " WordList(['together'])]"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "blob.ngrams(n=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc17e05a",
      "metadata": {
        "id": "dc17e05a"
      },
      "outputs": [],
      "source": [
        "# Textblob sentiment analysis\n",
        "\n",
        "import pandas as pd\n",
        "from textblob import TextBlob\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "import re\n",
        "tokenizer = ToktokTokenizer()\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm', disable=['ner'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e35bdaf8",
      "metadata": {
        "id": "e35bdaf8",
        "outputId": "1297d0d7-a4ca-4e0b-c133-5a48b1ddf45c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Sentiment(polarity=-0.9099999999999998, subjectivity=0.8666666666666667)"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "TextBlob(\"he is very bad boy\").sentiment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9506a340",
      "metadata": {
        "id": "9506a340"
      },
      "outputs": [],
      "source": [
        "# Polarity and Subjectivity\n",
        "# Polarity is a float value which helps in identifying whether a sentence is positive or negative.\n",
        "# Its values ranges in [-1,1] where 1 means positive statement and -1 means a negative statement.\n",
        "\n",
        "# On the other side, Subjective sentences generally refer to personal opinion, emotion or judgment whereas objective\n",
        "# refers to factual information. Subjectivity is also a float which lies in the range of [0,1].\n",
        "# Closer the value to 1, more likly it is public opinion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fef38b3",
      "metadata": {
        "id": "8fef38b3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18b99216",
      "metadata": {
        "id": "18b99216"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}